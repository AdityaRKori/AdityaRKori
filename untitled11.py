# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10fRzoYBll5kMA8bzQ9hU3GGxwS31tJrh
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# This line was previously in a separate cell and caused the error.
# Moving it to the same cell where 'pandas' is imported as 'pd' fixes the problem.
df = pd.read_csv('/content/heart_dataset_mini.csv')

df.head()

df.shape

df.columns

df.dtypes

df = df.drop_duplicates(subset=['Patient id'])

def remove_outliers(df, col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df_filtered = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
    return df_filtered

df = remove_outliers(df, 'age')
df = remove_outliers(df, 'chol')

df.isnull().sum()

category_cols=df.select_dtypes(include='object').columns
for col in category_cols:
  print(df[col].unique())

# prompt: print unique values

for col in category_cols:
  print(f'Unique values for {col}: {df[col].unique()}')

for col in ['age', 'chol']:
  df[col] = df[col].fillna(df[col].mean())

df = df.drop(columns=['Patient id'])

df.head()

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
for i in ['age','chol']:
  df[i] = scaler.fit_transform(df[[i]])

df

# prompt: plot scatter graph of standardization of given dataset

import matplotlib.pyplot as plt
plt.figure(figsize=(8, 6))
sns.scatterplot(x='age', y='chol', data=df)
plt.title('Scatter Plot of Standardized Age vs. Cholesterol')
plt.xlabel('Standardized Age')
plt.ylabel('Standardized Cholesterol')
plt.show()

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

model=KMeans(n_clusters=3)
model=model.fit(df)
k_labels=model.labels_
k_centroids=model.cluster_centers_
print(f"Silhouette Score: {silhouette_score(df,k_labels)}")

plt.figure(figsize=(8, 6))
# Accessing columns by name instead of NumPy-like indexing
plt.scatter(df['age'], df['chol'], c=k_labels, cmap='viridis')
plt.title(f'KMeans Clustering with K={3}')
plt.xlabel('Scaled Chol')
plt.ylabel('Scaled Age')
plt.legend()
plt.show()

df['cluster_labels'] = k_labels

df

cluster_stats = df.groupby('cluster_labels').agg(age_mean=('age','mean'), chol_mean=('chol','mean'))
cluster_stats

import numpy as np
def classify_new_datapoint(new_datapoint, scaler, model):
  scaled_new_datapoint = scaler.transform(np.array(new_datapoint).reshape(1, -1))
  predicted_cluster = model.predict(scaled_new_datapoint)[0]
  return predicted_cluster

  new_datapoint = [80,200]
  predicted_cluster = classify_new_datapoint(new_datapoint, scaler, model)
  print(f"The new datapoint belongs to cluster: (predicted_cluster)")

# prompt: cluster plot for each k

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# ... (Your existing code for data loading and preprocessing)

# Iterate through different values of k
for k in range(2, 11):  # Test k values from 2 to 10
    kmeans = KMeans(n_clusters=k, random_state=0)  # Use random_state for reproducibility
    kmeans.fit(df[['age', 'chol']])  # Fit KMeans on 'age' and 'chol' columns only
    labels = kmeans.labels_

    # Calculate and print silhouette score
    silhouette_avg = silhouette_score(df[['age', 'chol']], labels)
    print(f"For n_clusters = {k}, the average silhouette_score is : {silhouette_avg}")

    # Plot the clusters
    plt.figure(figsize=(8, 6))
    plt.scatter(df['age'], df['chol'], c=labels, cmap='viridis', s=50)
    plt.title(f'KMeans Clustering with K={k}')
    plt.xlabel('Scaled Age')
    plt.ylabel('Scaled Chol')
    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', label='Centroids')
    plt.legend()
    plt.show()

# prompt: normalise numerical values

# Existing code from the previous response is assumed to be present here.
# ... (Your existing code for data loading and preprocessing)

# Normalize 'age' and 'chol' columns using MinMaxScaler
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df[['age', 'chol']] = scaler.fit_transform(df[['age', 'chol']])

# ... (Rest of your existing code)

# prompt: standardise values

# Assuming 'df' is your DataFrame with 'age' and 'chol' columns already loaded and preprocessed.

from sklearn.preprocessing import StandardScaler

# Select the columns you want to standardize
columns_to_standardize = ['age', 'chol']

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the selected columns
df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])

# Now the 'age' and 'chol' columns in your DataFrame 'df' are standardized.

# prompt: import to jason

import json

# Assuming 'df' is your DataFrame as defined in the previous code.

# Convert the DataFrame to a JSON string
df_json = df.to_json(orient='records')  # You can choose other 'orient' values if needed

# Now you have the DataFrame in JSON format in the 'df_json' variable.
# You can save it to a file or use it as needed.

# Example: Save the JSON data to a file
with open('heart_data.json', 'w') as f:
    json.dump(df_json, f, indent=2)  # Use indent for pretty printing


# Example: Print the JSON string to the console (optional)
df_json

# prompt: get joblib code for k value and clusters

from joblib import dump, load

# ... (Your existing code for data loading, preprocessing, and KMeans clustering)

# Assuming 'kmeans' is your trained KMeans model and 'k' is the optimal number of clusters
# Save the trained model to a file
dump(kmeans, 'kmeans_model.joblib')

# Later, you can load the saved model
loaded_kmeans = load('kmeans_model.joblib')

# Use the loaded model to predict clusters for new data
# Example:
# new_data = [[new_age, new_chol]]  # Replace with your new data
# predicted_cluster = loaded_kmeans.predict(new_data)

# Access cluster centers
cluster_centers = loaded_kmeans.cluster_centers_
print(f"Cluster Centers for k={k}:\n{cluster_centers}")

# prompt: get a code for standardized values scaler joblib

dump(scaler, 'scaler.joblib')

# prompt: install pyngrock

!pip install pyngrok

# prompt: give a streamlit code for age and cholestrol
streamlit_code = """
import streamlit as st
import pandas as pd
import numpy as np
import joblib
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Load your pre-trained model and scaler (replace with your actual loading method)
loaded_model = joblib.load('kmeans_model.joblib')

scaler = joblib.load('scaler.joblib')

# Example: Assuming you've loaded the model and scaler into 'model' and 'scaler' variables.

# Streamlit app
st.title("chol,age Patient Clustering")

# Input fields for age and cholesterol
age = st.number_input("Enter age:", min_value=0, max_value=120, value=50)
chol = st.number_input("Enter chol level:", min_value=0, max_value=600, value=200)

# Button to trigger prediction
if st.button("Predict Cluster"):
    # Prepare the new data point
    new_patient_data = [age, chol]

    # Function to classify the new data point (same as before)
    def classify_new_datapoint(new_datapoint, model, scaler):
        new_datapoint_scaled = scaler.transform(np.array(new_datapoint).reshape(1, -1))
        predicted_cluster = model.predict(new_datapoint_scaled)[0]
        return predicted_cluster

    # Make the prediction
    predicted_cluster = classify_new_datapoint(new_patient_data, loaded_model, scaler)

    # Display the result
    st.write(f"The patient belongs to cluster: {predicted_cluster}")
"""
#save the code to a file
with open('app.py','w')as f:
  f.write(streamlit_code)
# The streamlit code is saved to app.py.  You should now run this app with streamlit run app.py

!streamlit run app.py &>/content/logs.txt &

from pyngrok import ngrok
ngrok.set_auth_token("2tqLkAmewm6lJ8FR1w7QDzyLPYF_5ym1o2fHCn9tP6a2ibLQT")

from pyngrok import ngrok

public_url = ngrok.connect(8501)
print(f"Streamlit App is live at: {public_url}")

!pkill -f ngrok