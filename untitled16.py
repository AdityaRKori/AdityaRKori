# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yQDDULLZJHxyxo6mZq2PdB1IbQLAS5Ha
"""

import numpy as np
import tensorflow as tf

a = tf.Variable(1)
b = tf.Variable(10)
c = a+b
print(c)
print(c.numpy())

@tf.function
def add(a,b):
  return a + b

def sub(a,b):
  return a - b

  a = tf.Variable(5)
  b = tf.Variable(6)
  c = tf.Variable(1)
  x = add(a,sub(b,c))
  print(x.numpy())

a=tf.Variable([[1,2],[3,4]], dtype=tf.float32)
b=tf.Variable([[7,8],[9,10]], dtype=tf.float32)
c=tf.Variable([[11,12],[13,14]], dtype=tf.float32)

y = (a*b) + c
print(y.numpy())

y+10

a.shape

a=tf.Variable([[1,2],[3,4],[5,6]], dtype=tf.float32)
a.shape

a = tf.Variable([[[1, 2, 3],
                 [3, 4, 4],
                 [5, 6, 8]],
                  [[1, 2, 3],
                 [3, 10, 4],
                 [5, 6, 8]]], dtype=tf.float32)
a.shape

a[1][1][1:]

A = tf.constant([[1, 2],[3, 4]])
B = tf.constant([[5, 6],[7, 8]])

C = tf.matmul(A, B)
print(C.numpy())

tf.nn.sigmoid(x)

import matplotlib.pyplot as plt

x = np.linspace(-5, 5, 100)

activations = {
    "Sigmoid": tf.nn.sigmoid(x),
    "Tanh": tf.nn.tanh(x),
    "ReLU": tf.nn.relu(x),
    "Leaky ReLU": tf.nn.leaky_relu(x, alpha=0.1)
}
plt.figure(figsize=(12,8))
for i, (name, y) in enumerate(activations.items()):
  plt.subplot(3, 3, i+1)
  plt.plot(x, y, label=name, color='b')
  plt.title(name)
  plt.axhline(0, color='gray', linestyle='--', linewidth=0.5)
  plt.axvline(0, color='gray', linestyle='--', linewidth=0.5)
  plt.legend()

plt.tight_layout()
plt.show()

x = np. linspace(-3, 3, 100)
inputs = np.vstack([x, x - 1, x + 1])

softmax_outputs = tf.nn.softmax(inputs, axis=0).numpy()

import pandas as pd
df1= pd.DataFrame(inputs.T)
df2= pd.DataFrame(softmax_outputs.T)

df1.head()

import matplotlib.pyplot as plt

x = tf.Variable([-3., -2., -1., 0., 1., 2., 3.])
def f(x):
  return x ** 2

  with tf.GradientTape() as tape:
    y = f(x)

  dfdx = tape.gradientTape(y, x)

  print("f(x):", y.numpy())
  print("df/dx:", dfdx.numpy())

  plt.plot(f(x), label="f(x) = x^2")
  plt.plot(dfdx, label="df/dx = 2x")
  plt.legend()
  plt.xlabel("x")
  plt.ylabel("y")
  plt.title("Function and it's Derivation")
  plt.grid(True)
  plt.show()

!pip install --upgrade tensorflow

import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf

x = tf.Variable(np.linspace(-10, 10, 100))

def sig(x):
  return 1 / (1 + tf.exp(-x))

with tf.GradientTape() as t:  # Remove persistence=True if using older TF version
  y = sig(x)

z = t.gradient(y, x)

plt.plot(sig(x))
plt.show()
plt.plot(x, z) # Plot the gradient
plt.show()

x = tf.Variable(np.linspace(-5,5,100), dtype=tf.float32)

# define activation function
@tf.function
def sigmoid(x):
  return tf.nn.sigmoid(x)

@tf.function
def tanh(x):
  return tf.nn.tanh(x)

@tf.function
def ReLU(x):
  return tf.nn.relu(x)

@tf.function
def leaky_relu(x):
  return tf.nn.leaky_relu(x)

# compute gradients using tf.GradientTape()
@tf.function
def compute_gradient(func, x):
  with tf.GradientTape() as tape:
    y = func(x)
  return tape.gradient(y,x)

# compute activation values
activation = {
    "Sigmoid": sigmoid(x),
    "Tanh": tanh(x),
    "ReLU": ReLU(x),
    "LeakyReLU": leaky_relu(x)
}

# compute gradients
tf.gradients = {
    "Sigmoid": compute_gradient(sigmoid, x),
    "Tanh": compute_gradient(tanh, x),
    "ReLU": compute_gradient(ReLU, x),
    "LeakyReLU": compute_gradient(leaky_relu, x)
}

x_vals = x.numpy()
activations = {k: v.numpy() for k, v in activation.items()}
gradients = {k: v.numpy() for k, v in tf.gradients.items()}

#plot both activations functions and their gradients on the same graph
plt.figure(figsize=(12,8))
for i,(name,y) in enumerate(activations.items()):
  plt.subplot(2,2,i+1)#2x2 grid layout
  plt.plot(x_vals,y,label=f'{name}',color='b')#blue for activation function
  plt.plot(x_vals,gradients[list(gradients.keys())[i]],label=f'{name} Gradient',color='r')#red for gradient
  plt.title(f"{name} & Gradient")
  plt.axhline(0,color='grey',linestyle='--',linewidth=0.5)
  plt.axvline(0,color='grey',linestyle='--',linewidth=0.5)
  plt.legend()
plt.tight_layout()
plt.show()

x = tf.Variable(2.0)

@tf.function
def chain_rule_example(x):
  with tf.GradientTape(persistent=True) as tape:
    y = x ** 2
    z = tf.sin(y)

    dz_dy = tape.gradient(z, y)
    dy_dx = tape.gradient(y, x)

  dz_dx = dz_dy * dy_dx
  # Return both x and dz_dx
  return x, dz_dx # Changed this line to return both x and the gradient.

# Assign the returned values to x_value and dz_dx_value
x_value, dz_dx_value = chain_rule_example(x)

# Use the correct variable name z_value
print(f"z = sin(x^2) at x=2: {tf.sin(x_value**2).numpy()}") # Changed this line to calculate z correctly and use x_value
print(f"dz/dx using Chain Rule: {dz_dx_value.numpy()}")

import tensorflow as tf
import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Input,BatchNormalization,Dropout

iris = load_iris()

iris

x = iris.data
y1 = iris.target

x

y1

x = iris.data
y = iris.target.reshape

y

x = iris.data
y = iris.target.reshape(-1, 1) # Call the reshape method with desired shape
encoder = OneHotEncoder(sparse_output=False)
y_encoded = encoder.fit_transform(y)

y_encoded

x_train, x_test, y_train, y_test = train_test_split(x, y_encoded, test_size=0.2, random_state=42)

x_train.shape

scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

model = Sequential([
    Dense(3, activation='softmax', input_shape=(4,))
])

model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(x_train, y_train, epochs=10, verbose=1, validation_split=0.2)

test_loss, test_acc = model.evaluate(x_test, y_test, verbose=1)
print(f"\nTest accuracy: {test_acc:.4f}")

predictions = model.predict(x_test)

predictions

predict_classes = np.argmax(predictions, axis=1)

predict_classes

actual_classes = np.argmax(y_test, axis=1)

actual_classes

print("\nPredictions vs Actual Labels:")
for i in range(3):
  print(f"Predicted: {predict_classes[i]}, Actual: {actual_classes[i]}")

from sklearn.metrics import classification_report
report = classification_report(actual_classes, predict_classes)
print("Classification Report:\n", report)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

conf_matrix = confusion_matrix(actual_classes, predict_classes)

plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix")
plt.show()

history

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

model= Sequential([
    Dense(8,activation='relu',input_shape=(4,)),
    Dense(3,activation='softmax')
])

model= Sequential()
model.add(Input(shape=(4,)))
model.add(Dense(8,activation='sigmoid'))
model.add(Dense(3,activation='softmax'))

model.compile(loss ='categorical_crossentropy', metrics=['accuracy'],
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.05))
history = model.fit(x_train, y_train, epochs=50, batch_size = 0, validation_data= ((x_test, y_test)))
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=1)
print(f"\nTest accuracy: {test_acc:.4f}")

model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(x_train, y_train, epochs=10, verbose=1, validation_split=0.2)

model = Sequential([
   Input(shape=(4,)),
   Dense(32, activation='relu'),
   BatchNormalization(),
   Dropout(0.2),
   Dense(16, activation='relu'),
   Dense(3, activation='softmax')
])

model.compile(loss='categorical_crossentropy', metrics=['accuracy'],
               optimizer=tf.keras.optimizers.SGD(learning_rate=0.05))
history= model.fit(x_train, y_train, epochs=50, batch_size=8, validation_data =((x_test,y_test)))
#evaluate model accuracy
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=1)
print(f"\nTest accuracy: {test_acc:.4f}")

model.save('iris_model.h5')
print("\nModel saved successfully!")

!pip install streamlit pyngrok

# Commented out IPython magic to ensure Python compatibility.
# # prompt: build a dashboard using streamlit to predict the petals
# 
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import numpy as np
# import tensorflow as tf
# from tensorflow.keras.models import load_model
# 
# # Load the trained model
# model = load_model('iris_model.h5')
# 
# # Create the Streamlit app
# st.title("Iris Flower Petal Prediction")
# 
# # Get user input
# sepal_length = st.number_input("Sepal Length (cm)", min_value=0.0, max_value=10.0, value=5.0)
# sepal_width = st.number_input("Sepal Width (cm)", min_value=0.0, max_value=10.0, value=3.0)
# petal_length = st.number_input("Petal Length (cm)", min_value=0.0, max_value=10.0, value=4.0)
# petal_width = st.number_input("Petal Width (cm)", min_value=0.0, max_value=10.0, value=2.0)
# 
# # Create a button to make the prediction
# if st.button("Predict"):
#   # Create a numpy array from the user input
#   input_data = np.array([[sepal_length, sepal_width, petal_length, petal_width]])
# 
#   # Make the prediction
#   prediction = model.predict(input_data)
# 
#   # Get the predicted class
#   predicted_class = np.argmax(prediction)
# 
#   # Map the predicted class to the flower species
#   if predicted_class == 0:
#     flower_species = "Setosa"
#   elif predicted_class == 1:
#     flower_species = "Versicolor"
#   else:
#     flower_species = "Virginica"
# 
#   # Display the prediction
#   st.write(f"The predicted flower species is: **{flower_species}**")
# 
# 
#

# prompt: run the streamlit code

!streamlit run app.py &>/dev/null&
!pip install pyngrok
from pyngrok import ngrok

# Terminate open tunnels if exist
ngrok.kill()

# Setting the authtoken (optional)
# Get your authtoken from https://dashboard.ngrok.com/auth
NGROK_AUTH_TOKEN = "2tqLkAmewm6lJ8FR1w7QDzyLPYF_5ym1o2fHCn9tP6a2ibLQT"  # Replace with your authtoken
ngrok.set_auth_token(NGROK_AUTH_TOKEN)

# Open an HTTPs tunnel on port 8501 for Streamlit
# Changed from ngrok.connect(port='8501') to the following:
tunnel = ngrok.connect(8501, "http") # Specify port and protocol
public_url = tunnel.public_url
print(f"Your Streamlit app is available at: {public_url}")

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

np.random.seed(42)
x=np.random.rand(200, 1)* 10
y = 3*x + np.random.rand(200, 1)*2 # Corrected: Using np.random.rand to generate random numbers

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

model= tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(1,)),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1)
])

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),
              loss='mean_squared_error',
              metrics=['mae'])

history = model.fit(x_train, y_train, epochs=50, batch_size=10, verbose=1, )

test_loss, test_mae = model.evaluate(x_test, y_test, verbose=1)
print(f"\nTest MAE: {test_mae:.4f}")

y_pred = model.predict(x_test)

!pkill -f ngrok

